<!DOCTYPE html>
<html lang="en-US">
  <head>

    
    
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Trustworthy and Socially Responsible Machine Learning | Workshop at NeurIPS 2022</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Trustworthy and Socially Responsible Machine Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Trustworthy and Socially Responsible Machine Learning | Workshop at NeurIPS 2022" />
<meta property="og:description" content="Trustworthy and Socially Responsible Machine Learning | Workshop at NeurIPS 2022" />
<meta property="og:site_name" content="Trustworthy and Socially Responsible Machine Learning" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Trustworthy and Socially Responsible Machine Learning" />
<script type="application/ld+json">
{"headline":"Trustworthy and Socially Responsible Machine Learning","url":"/schedule.html","description":"Trustworthy and Socially Responsible Machine Learning | Workshop at NeurIPS 2022","@type":"WebPage","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=">
  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">Trustworthy and Socially Responsible <br> Machine Learning (TSRML)</h1>
      <h2 class="project-tagline">Workshop at <a href="https://neurips.cc/">NeurIPS 2022</a><br> Virtual, December 9, 2022</h2>
      
      
      <a href="/" class="btn">Home</a>
      
      <a href="/cfp" class="btn">Call for Papers</a>
      
      <a href="/papers" class="btn">Accepted Papers</a>
      
      <a href="/schedule" class="btn">Schedule</a>
      
      <a href="/speakers" class="btn">Speakers</a>
      
      <a href="/organizers" class="btn">Organizers</a>
      
      <a href="/committee" class="btn">Program Committee</a>
      
      <a href="/related" class="btn">Related Workshops</a>
      
    </header>

    <main id="content" class="main-content" role="main">
      <h1>Instructions for Authors with Accepted Papers</h1>

        <h4>Video</h4>

        The recommended length of the pre-recorded video is 10 minutes. 
        You should have received the uploading link from slideslive.com via email and that link is the only entrance to upload the video. 
        If you have not received the link yet, please let us know.
        The video uploading deadline is Nov 10, AoE.

        <h4>Poster and Thumbnail</h4>

        You should have received a link from neurips.com.
        You will need to upload your poster and thumbnail following the link. 
        They will be used by the conference website.
        The poster and thumbnail uploading deadline will be in December, a few days before the workshop date (December 9, 2022).
        In addition, our workshop will have a virtual poster session on Topia. We are working on setting up the virtual poster session and more instructions will be sent out soon.

        <h4>Camera-Ready Version</h4>

        Please change “\usepackage{tsrml_2022}” to “\usepackage[final]{tsrml_2022}” and populate the author field when preparing the camera-ready version. 
        There is no strict page limit for the camera-ready version, but we recommend keeping the main text within 6 pages. Please submit your camera-ready version by clicking “Camera-Ready Revision” in OpenReview.
        The deadline for camera-ready version submission is Nov 10, AoE.

        We plan to make the camera-ready version and forum discussion public for accepted papers. 
        Hence, if you see factual misunderstandings or questions from the review, we encourage you to post replies by clicking "Official Comment". Please follow <a href="https://nips.cc/public/CodeOfConduct">NeurIPS Code of Conduct</a> to involve in the discussion.

      <h1 id="accepted-papers">Accepted Papers</h1>

        <h4>This list will be updated based on the metadata of each paper after the camera-ready deadline.</h4>
<ol>
  <li><b>Take 5: Interpretable Image Classification with a Handful of Features</b> <br /> Thomas Norrenbrock; Marco Rudolph; Bodo Rosenhahn</li>
        
  <li><b>Membership Inference Attacks via Adversarial Examples</b> <br /> Hamid Jalalzai; Elie Kadoche; Rémi Leluc; Vincent Plassier</li>
  
  <li><b>Scalable and Improved Algorithms for Individually Fair Clustering</b> <br /> Mohammadhossein Bateni; Vincent Cohen-Addad; Alessandro Epasto; Silvio Lattanzi</li>
  
  <li><b>Not All Knowledge Is Created Equal: Mutual Distillation of Confident Knowledge</b> <br /> ZIYUN LI; Xinshao Wang; Christoph Meinel; Neil M. Robertson; David A. Clifton; Haojin Yang</li>
  
  <li><b>Just Following AI Orders: When Unbiased People Are Influenced By Biased AI</b> <br /> Hammaad Adam; Aparna Balagopalan; Emily Alsentzer; Fotini Christia; Marzyeh Ghassemi</li>
  
  <li><b>Towards Algorithmic Fairness in Space-Time: Filling in Black Holes</b> <br /> Cheryl Brooks; Aritra Guha; Subhabrata Majumdar; Divesh Srivastava; Zhengyi Zhou</li>
  
  <li><b>COVID-Net Biochem: An Explainability-driven Framework to Building Machine Learning Models for Predicting Survival and Kidney Injury of COVID-19 Patients from Clinical and Biochemistry Data</b> <br /> Hossein Aboutalebi; Maya Pavlova; Mohammad Javad Shafiee; Adrian Florea; Andrew Hryniowski; Alexander Wong</li>
  
  <li><b>On the Feasibility of Compressing Certifiably Robust Neural Networks</b> <br /> Pratik Vaishnavi; Veena Krish; Farhan Ahmed; Kevin Eykholt; Amir Rahmati</li>
  
  <li><b>Differentially Private Bias-Term only Fine-tuning of Foundation Models</b> <br /> Zhiqi Bu; Yu-Xiang Wang; Sheng Zha; George Karypis</li>
  
  <li><b>When Fairness Meets Privacy: Fair Classification with Semi-Private Sensitive Attributes</b> <br /> Canyu Chen; Yueqing Liang; Xiongxiao Xu; Shangyu Xie; Yuan Hong; Kai Shu</li>
  
  <li><b>Visual Prompting for Adversarial Robustness</b> <br /> Aochuan Chen; Peter Lorenz; Yuguang Yao; Pin-Yu Chen; Sijia Liu</li>
  
  <li><b>Is the Next Winter Coming for AI?The Elements of Making Secure and Robust AI</b> <br /> Joshua Harguess</li>
  
  <li><b>Attack-Agnostic Adversarial Detection</b> <br /> Jiaxin Cheng; Mohamed E. Hussein; Jayadev Billa; Wael AbdAlmgaeed</li>
  
  <li><b>Provable Re-Identification Privacy</b> <br /> Zachary Izzo; Jinsung Yoon; Sercan O Arik; James Zou</li>
  
  <li><b>Anonymization for Skeleton Action Recognition</b> <br /> Saemi Moon; Myeonghyeon Kim; Zhenyue Qin; Yang Liu; Dongwoo Kim</li>
  
  <li><b>Men Also Do Laundry: Multi-Attribute Bias Amplification</b> <br /> Dora Zhao; Jerone Theodore Alexander Andrews; Alice Xiang</li>
  
  <li><b>Cold Posteriors through PAC-Bayes</b> <br /> Konstantinos Pitas; Julyan Arbel</li>
  
  <li><b>Certified Defences Against Adversarial Patch Attacks on Semantic Segmentation</b> <br /> Maksym Yatsura; Kaspar Sakmann; N. Grace Hua; Matthias Hein; Jan Hendrik Metzen</li>
  
  <li><b>Bias Amplification in Image Classification</b> <br /> Melissa Hall; Laurens van der Maaten; Laura Gustafson; Maxwell Jones; Aaron Bryan Adcock</li>
  
  <li><b>Hybrid-EDL: Improving Evidential Deep Learning for Uncertainty Quantification on Imbalanced Data</b> <br /> Tong Xia; Jing Han; Lorena Qendro; Ting Dang; Cecilia Mascolo</li>
  
  <li><b>Indiscriminate Data Poisoning Attacks on Neural Networks</b> <br /> Yiwei Lu; Gautam Kamath; Yaoliang Yu</li>
  
  <li><b>Finding Safe Zones of Markov Decision Processes Policies</b> <br /> Michal Moshkovitz; Lee Cohen; Yishay Mansour</li>
  
  <li><b>On the Importance of Architectures and Hyperparameters for Fairness in Face Recognition</b> <br /> Samuel Dooley; Rhea Sanjay Sukthanker; John P Dickerson; Colin White; Frank Hutter; Micah Goldblum</li>
  
  <li><b>Poisoning Generative Models to Promote Catastrophic Forgetting</b> <br /> Siteng Kang; Xinhua Zhang</li>
  
  <li><b>On Causal Rationalization</b> <br /> Wenbo Zhang; TONG WU; Yunlong Wang; Yong Cai; Hengrui Cai</li>
  
  <li><b>A View From Somewhere: Human-Centric Face Representations</b> <br /> Jerone Theodore Alexander Andrews; Przemyslaw Joniak; Alice Xiang</li>
  
  <li><b>REGLO: Provable Neural Network Repair for Global Robustness Properties</b> <br /> Feisi Fu; Zhilu Wang; Jiameng Fan; Yixuan Wang; Chao Huang; Xin Chen; Qi Zhu; Wenchao Li</li>
  
  <li><b>What Makes a Good Explanation?: A Unified View of Properties of Interpretable ML</b> <br /> Zixi Chen; Varshini Subhash; Marton Havasi; Weiwei Pan; Finale Doshi-Velez</li>
  
  <li><b>On the Impact of Adversarially Robust Models on Algorithmic Recourse</b> <br /> Satyapriya Krishna; Chirag Agarwal; Himabindu Lakkaraju</li>
  
  <li><b>Participatory Systems for Personalized Prediction</b> <br /> Hailey James; Chirag Nagpal; Katherine A Heller; Berk Ustun</li>
  
  <li><b>TalkToModel: Explaining Machine Learning Models with Interactive Natural Language Conversations</b> <br /> Dylan Z Slack; Satyapriya Krishna; Himabindu Lakkaraju; Sameer Singh</li>
  
  <li><b>Differentially Private Gradient Boosting on Linear Learners for Tabular Data</b> <br /> Saeyoung Rho; Shuai Tang; Sergul Aydore; Michael Kearns; Aaron Roth; Yu-Xiang Wang; Steven Wu; Cedric Archambeau</li>
  
  <li><b>A Deep Dive into Dataset Imbalance and Bias in Face Identification</b> <br /> Valeriia Cherepanova; Steven Reich; Samuel Dooley; Hossein Souri; John P Dickerson; Micah Goldblum; Tom Goldstein</li>
  
  <li><b>Evaluating the Practicality of Counterfactual Explanation</b> <br /> Nina Spreitzer; Hinda Haned; Ilse van der Linden</li>
  
  <li><b>Certified Training: Small Boxes are All You Need</b> <br /> Mark Niklas Mueller; Franziska Eckert; Marc Fischer; Martin Vechev</li>
  
  <li><b>Group Excess Risk Bound of Overparameterized Linear Regression with Constant-Stepsize SGD</b> <br /> Arjun Subramonian; Levent Sagun; Kai-Wei Chang; Yizhou Sun</li>
  
  <li><b>Strategy-Aware Contextual Bandits</b> <br /> Keegan Harris; Chara Podimata; Steven Wu</li>
  
  <li><b>Addressing Bias in Face Detectors using Decentralised Data collection with incentives</b> <br /> Ahan M R; Robin Lehmann; Richard Blythman</li>
  
  <li><b>Learning to Take a Break: Sustainable Optimization of Long-Term User Engagement</b> <br /> Eden Saig; Nir Rosenfeld</li>
  
  <li><b>Explainability in Practice: Estimating Electrification Rates from Mobile Phone Data in Senegal</b> <br /> Laura State; Hadrien Salat; Stefania Rubrichi; Zbigniew Smoreda</li>
  
  <li><b>Distributed Differential Privacy in Multi-Armed Bandits</b> <br /> Sayak Ray Chowdhury; Xingyu Zhou</li>
  
  <li><b>Individual Privacy Accounting with Gaussian Differential Privacy</b> <br /> Antti Koskela; Marlon Tobaben; Antti Honkela</li>
  
  <li><b>Hidden Poison: Machine Unlearning Enables Camouflaged Poisoning Attacks</b> <br /> Jimmy Z. Di; Jack Douglas; Jayadev Acharya; Gautam Kamath; Ayush Sekhari</li>
  
  <li><b>PINTO: Faithful Language Reasoning Using Prompt-Generated Rationales</b> <br /> PeiFeng Wang; Aaron Chan; Filip Ilievski; Muhao Chen; Xiang Ren</li>
  
  <li><b>zPROBE: Zero Peek Robustness Checks for Federated Learning</b> <br /> Zahra Ghodsi; Mojan Javaheripi; Nojan Sheybani; Xinqiao Zhang; Ke Huang; Farinaz Koushanfar</li>
  
  <li><b>A Theory of Learning with Competing Objectives and User Feedback</b> <br /> Pranjal Awasthi; Corinna Cortes; Yishay Mansour; Mehryar Mohri</li>
  
  <li><b>Accelerating Open Science for AI in Heliophysics</b> <br /> Dolores Garcia; Paul wright; Mark CM Cheung; Meng Jin; James Parr</li>
  
  <li><b>FL-Talk: Covert Communication in Federated Learning via Spectral Steganography</b> <br /> Huili Chen; Farinaz Koushanfar</li>
  
  <li><b>Honest Students from Untrusted Teachers: Learning an Interpretable Question-Answering Pipeline from a Pretrained Language Model</b> <br /> Jacob Eisenstein; Daniel Andor; Bernd Bohnet; Michael Collins; David Mimno</li>
  
  <li><b>Just Avoid Robust Inaccuracy: Boosting Robustness Without Sacrificing Accuracy</b> <br /> Yannick Merkli; Pavol Bielik; PETAR TSANKOV; Martin Vechev</li>
  
  <li><b>Interactive Rationale Extraction for Text Classification</b> <br /> Jiayi Dai; Mi-Young Kim; Randy Goebel</li>
  
  <li><b>Controllable Attack and Improved Adversarial Training in Multi-Agent Reinforcement Learning</b> <br /> Xiangyu Liu; Souradip Chakraborty; Furong Huang</li>
  
  <li><b>Few-shot Backdoor Attacks via Neural Tangent Kernels</b> <br /> Jonathan Hayase; Sewoong Oh</li>
  
  <li><b>Information-Theoretic Evaluation of Free-Text Rationales with Conditional $\mathcal{V}$-Information</b> <br /> Hanjie Chen; Faeze Brahman; Xiang Ren; Yangfeng Ji; Yejin Choi; Swabha Swayamdipta</li>
  
  <li><b>Uncertainty-aware predictive modeling for fair data-driven decisions</b> <br /> Patrick Kaiser; Christoph Kern; David Rügamer</li>
  
  <li><b>GFairHint: Improving Individual Fairness for Graph Neural Networks via Fairness Hint</b> <br /> Paiheng Xu; Yuhang Zhou; Bang An; Wei Ai; Furong Huang</li>
  
  <li><b>Cooperation or Competition: Avoiding Player Domination for Multi-target Robustness by Adaptive Budgets</b> <br /> Yimu Wang; Dinghuai Zhang; Yihan Wu; Heng Huang; Hongyang Zhang</li>
  
  <li><b>Revisiting Robustness in Graph Machine Learning</b> <br /> Lukas Gosch; Daniel Sturm; Simon Geisler; Stephan Günnemann</li>
  
  <li><b>A Closer Look at the Intervention Procedure of Concept Bottleneck Models</b> <br /> Sungbin Shin; Yohan Jo; Sungsoo Ahn; Namhoon Lee</li>
  
  <li><b>Striving for data-model efficiency: Identifying data externalities on group performance</b> <br /> Esther Rolf; Ben Packer; Alex Beutel; Fernando Diaz</li>
  
  <li><b>Physically-Constrained Adversarial Attacks on Brain-Machine Interfaces</b> <br /> Xiaying Wang; Rodolfo Octavio Siller Quintanilla; Michael Hersche; Luca Benini; Gagandeep Singh</li>
  
  <li><b>Training Differentially Private Graph Neural Networks with Random Walk Sampling</b> <br /> Morgane Ayle; Jan Schuchardt; Lukas Gosch; Daniel Zügner; Stephan Günnemann</li>
  
  <li><b>Forgetting Data from Pre-trained GANs</b> <br /> Zhifeng Kong; Kamalika Chaudhuri</li>
  
  <li><b>A Brief Overview of AI Governance for Responsible Machine Learning Systems</b> <br /> Navdeep Gill; Marcos V. Conde</li>
  
  <li><b>Private Data Leakage via Exploiting Access Patterns of Sparse Features in Deep Learning-based Recommendation Systems</b> <br /> Hanieh Hashemi; Wenjie Xiong; Liu Ke; Kiwan Maeng; Murali Annavaram; G. Edward Suh; Hsien-Hsin S. Lee</li>
  
  <li><b>Benchmarking the Effect of Poisoning Defenses on the Security and Bias of the Final Model</b> <br /> Nathalie Baracaldo; Kevin Eykholt; Farhan Ahmed; Yi Zhou; Shriti Priya; Taesung Lee; Swanand Kadhe; Yusong Tan; Sridevi Polavaram; Sterling Suggs</li>
  
  <li><b>A Fair Loss Function for Network Pruning</b> <br /> Robbie Meyer; Alexander Wong</li>
  
  <li><b>DensePure: Understanding Diffusion Models towards Adversarial Robustness </b> <br /> Chaowei Xiao; Zhongzhu Chen; Kun Jin; Jiongxiao Wang; Weili Nie; Mingyan Liu; Anima Anandkumar; Bo Li; Dawn Song</li>
  
  <li><b>Quantifying Social Biases Using Templates is Unreliable</b> <br /> Preethi Seshadri; Pouya Pezeshkpour; Sameer Singh</li>
  
  <li><b>Real world relevance of generative counterfactual explanations</b> <br /> Swami Sankaranarayanan; Thomas Hartvigsen; Lauren Oakden-Rayner; Marzyeh Ghassemi; Phillip Isola</li>
  
  <li><b>On the Robustness of deep learning-based MRI Reconstruction to image transformations</b> <br /> Jinghan Jia; Mingyi Hong; Yimeng Zhang; Mehmet Akcakaya; Sijia Liu</li>
  
  <li><b>Denoised Smoothing with Sample Rejection for Robustifying Pretrained Classifiers</b> <br /> Fatemeh Sheikholeslami; Wan-Yi Lin; Jan Hendrik Metzen; Huan Zhang; J Zico Kolter</li>
  
  <li><b>An Analysis of Social Biases Present in BERT Variants Across Multiple Languages</b> <br /> Parishad BehnamGhader; Aristides Milios</li>
  
  <li><b>When Personalization Harms: Reconsidering the Use of Group Attributes of Prediction</b> <br /> Vinith Menon Suriyakumar; Marzyeh Ghassemi; Berk Ustun</li>
  
  <li><b>Socially Responsible Reasoning with Large Language Models and The Impact of Proper Nouns</b> <br /> Sumit Kumar Jha; Rickard Ewetz; Alvaro Velasquez; Susmit Jha</li>
  
  <li><b>Fairness-aware Missing Data Imputation</b> <br /> Yiliang Zhang; Qi Long</li>
  
  <li><b>But Are You Sure? Quantifying Uncertainty in Model Explanations</b> <br /> Charles Thomas Marx; Youngsuk Park; Hilaf Hasson; Bernie Wang; Stefano Ermon; Luke Huan</li>
  
  <li><b>On the Trade-Off between Actionable Explanations and the Right to be Forgotten</b> <br /> Martin Pawelczyk; Tobias Leemann; Asia Biega; Gjergji Kasneci</li>
  
  <li><b>A Stochastic Optimization Framework for Fair Risk Minimization</b> <br /> Andrew Lowy; Sina Baharlouei; Rakesh Pavan; Meisam Razaviyayn; Ahmad Beirami</li>
  
  <li><b>Beyond Protected Attributes: Disciplined Detection of Systematic Deviations in Data</b> <br /> Adebayo Oshingbesan; Winslow Georgos Omondi; Girmaw Abebe Tadesse; Celia Cintas; Skyler Speakman</li>
  
  <li><b>Towards Reasoning-Aware Explainable VQA</b> <br /> Rakesh Vaideeswaran; Feng Gao; ABHINAV MATHUR; Govind Thattai</li>
  
  <li><b>Learning from uncertain concepts via test time interventions</b> <br /> Ivaxi Sheth; Aamer Abdul Rahman; Laya Rafiee Sevyeri; Mohammad Havaei; Samira Ebrahimi Kahou</li>
  
  <li><b>Generating Intuitive Fairness Specifications for Natural Language Processing</b> <br /> Florian E. Dorner; Momchil Peychev; Nikola Konstantinov; Naman Goel; Elliott Ash; Martin Vechev</li>
  
  <li><b>Assessing Performance and Fairness Metrics in Face Recognition - Bootstrap Methods</b> <br /> Jean-Rémy Conti; Stephan Clémençon</li>
  
  <li><b>Case Study: Applying Decision Focused Learning in the Real World</b> <br /> Shresth Verma; Aditya Mate; Kai Wang; Aparna Taneja; Milind Tambe</li>
  
  <li><b>Inferring Class Label Distribution of Training Data from Classifiers: An Accuracy-Augmented Meta-Classifier Attack</b> <br /> Raksha Ramakrishna; György Dán</li>
  
  <li><b>Improving Fairness in Image Classification via Sketching</b> <br /> Ruichen Yao; Ziteng Cui; Xiaoxiao Li; Lin Gu</li>
</ol>


<footer class="site-footer">
  <span class="site-footer-credits">
    <!-- Please contact <a href="mailto:huan@huan-zhang.com">Huan Zhang</a> or <a href="mailto:linyi2@illinois.edu">Linyi Li</a> if you have any questions.<br> -->
    If you have any questions, please contact the organizers via <a href="mailto:tsrml2022@googlegroups.com">tsrml2022@googlegroups.com</a>.<br>
    This page was generated by <a href="https://pages.github.com">GitHub Pages</a> following the template of <a href="https://aisecure-workshop.github.io/aml-iclr2021">AML-ICLR 2021 workshop</a>.
  </span>
</footer>
</main>
</body>
</html>

