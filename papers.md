---
layout: default
---

# Accepted Papers

- <b>[Efficient Disruptions of Black-box Image Translation Deepfake Generation Systems]("./assets/papers/1.pdf")</b> <br /> Nataniel Ruiz (Boston University); Sarah Bargal (Boston University); Stan Sclaroff (Boston University)
- <b>Bridging the Gap Between Adversarial Robustness and Optimization Bias</b> <br /> Fartash Faghri (University of Toronto); Cristina Vasconcelos (Google); David J Fleet (University of Toronto); Fabian Pedregosa (Google); Nicolas Le Roux (Google)
- <b>Covariate Shift Adaptation for Adversarially Robust Classifier</b> <br /> Jay Nandy (National University of Singapore); Sudipan Saha (Technical University of Munich); Wynne Hsu (National University of Singapore); Mong Li Lee (National University of Singapore); Xiaoxiang Zhu (Technical University of Munich,Germany)
- <b>Poisoned classifiers are not only backdoored, they are fundamentally broken</b> <br /> Mingjie Sun (Carnegie Mellon University); Siddhant Agarwal (Indian Institute of Technology, Kharagpur); Zico Kolter (Carnegie Mellon University)
- <b>Reliably fast adversarial training via latent adversarial perturbation</b> <br /> Geon Yeong Park (KAIST); Sang Wan Lee (KAIST)
- <b>Imbalanced Gradients: A New Cause of Overestimated Adversarial Robustness</b> <br /> Linxi Jiang (Fudan University); Xingjun Ma (Deakin University); Zejia Weng (Fudan University); James Bailey (THE UNIVERSITY OF MELBOURNE); Yu-Gang Jiang (Fudan University)
- <b>SHIFT INVARIANCE CAN REDUCE ADVERSARIAL ROBUSTNESS</b> <br /> David Jacobs (University of Maryland, USA); Ronen Basri (Weizmann Institute of Science); Vasu Singla (University Of Maryland); Songwei Ge (University of Maryland)
- <b>What is Wrong with One-Class Anomaly Detection?</b> <br /> JuneKyu Park (Ajou University); Jeong-Hyeon Moon (Ajou University); Namhyuk Ahn (Ajou University); Kyung-Ah Sohn (Ajou University)
- <b>Safe Model-based Reinforcement Learning with Robust Cross-Entropy Method</b> <br /> Zuxin Liu (Carnegie Mellon University); Hongyi Zhou (CMU); Baiming Chen (Tsinghua University); Sicheng Zhong (University of Toronto); DING ZHAO (Carnegie Mellon University)
- <b>GateNet: Bridging the gap between Binarized Neural Network and FHE evaluation</b> <br /> Cheng Fu (University of California, San Diego); Hanxian Huang (UC San Diego); Xinyun Chen (UC Berkeley); Jishen Zhao (University of California, San Diego)
- <b>High-Robustness, Low-Transferability Fingerprinting of Neural Networks</b> <br /> Siyue Wang (Northeastern University); Xiao Wang (Boston University); Pin-Yu Chen (IBM Research); Pu Zhao (Northeastern University); Xue Lin (Northeastern University)
- <b>Poisoning Deep Reinforcement Learning Agents with In-Distribution Triggers</b> <br /> Kiran Karra (JHU/APL); Chace Ashcraft (JHU/APL)
- <b>Non-Singular Adversarial Robustness of Neural Networks</b> <br /> Yu-Lin Tsai (National Chiao Tung University); Chia-Yi Hsu (National Yang Ming Chiao Tung University); Chia-Mu Yu (National Chiao Tung University); Pin-Yu Chen (IBM Research)
- <b>Adversarial Examples Make Stronger Poisons</b> <br /> Liam Fowl (University of Maryland); Micah Goldblum (University of Maryland, College Park); Ping-yeh Chiang (University of Maryland, College Park); Jonas A. Geiping (University of Siegen); Tom Goldstein (University of Maryland, College Park)
- <b>What Doesn't Kill You Makes You Robust(er): Adversarial Training against Poisons and Backdoors</b> <br /> Jonas A. Geiping (University of Siegen); Liam Fowl (University of Maryland); Gowthami Somepalli (University of Maryland); Micah Goldblum (University of Maryland); Michael Moeller (University of Siegen); Tom Goldstein (University of Maryland, College Park)
- <b>Byzantine-Robust and Privacy-Preserving Framework for FedML</b> <br /> Hanieh Hashemi (University of Southern California); Yongqin Wang (University of Southern California); Chuan Guo (Facebook AI Research); Murali Annavaram (University of Southern California)
- <b>Data Augmentation Can Improve Robustness</b> <br /> Sylvestre-Alvise Rebuffi (DeepMind); Sven Gowal (DeepMind); Dan Andrei Calian (DeepMind); Florian Stimberg (); Olivia Wiles (DeepMind); Timothy Arthur Mann (DeepMind)
- <b>Baseline Pruning-Based Approach to Trojan Detection in Neural Networks</b> <br /> Peter Bajcsy (NIST); Michael Majurski (NIST)
- <b>Extracting Hyperparameter Constraints From Code</b> <br /> Ingkarat Rak-amnouykit (Rensselaer Polytechnic Institute); Ana Milanova (Rensselaer Polytechnic Institute); Guillaume Baudart (Inria Paris, École normale supérieure - PSL University); Martin Hirzel (IBM Research); Julian Dolby (IBM Research)
- <b>Coordinated Attacks Against Federated Learning: A Multi-Agent Reinforcement Learning Approach</b> <br /> Wen Shen (Tulane University); Henger Li (Tulane University); Zizhan Zheng (Tulane University)
- <b>Regularization Can Help Mitigate Poisoning Attacks... with the Right Hyperparameters</b> <br /> Javier Carnerero-Cano (Imperial College London); Luis Muñoz-González (Imperial College London); Phillippa Spencer (Defence Science and Technology Laboratory); Emil Lupu (Imperial College London)
- <b>Ditto: Fair and Robust Federated Learning Through Personalization</b> <br /> Tian Li (Carnegie Mellon University); Shengyuan Hu (Carnegie Mellon University); Ahmad Beirami (Facebook AI); Virginia Smith (Carnegie Mellon University)
- <b>Low Curvature Activations Reduce Overfitting in Adversarial Training</b> <br /> Vasu Singla (University Of Maryland); Sahil Singla (University of Maryland); David Jacobs (University of Maryland, USA); Soheil Feizi (University of Maryland)
- <b>Examining Trends in Out-of-Domain Confidence</b> <br /> Hamza Qadeer (University of California, Berkeley); Michael Chau (University of California, Berkeley); Eric Zhu (University of California, Berkeley); Matthew A Wright (University of California Berkeley); Richard Liaw (UC Berkeley)
- <b>Doing More with Less: Improving Robustness using Generated Data</b> <br /> Sven Gowal (DeepMind); Sylvestre-Alvise Rebuffi (DeepMind); Olivia Wiles (DeepMind); Florian Stimberg (); Dan Andrei Calian (DeepMind); Timothy Arthur Mann (DeepMind)
- <b>Hidden Backdoor Attack against Semantic Segmentation Models</b> <br /> Yiming Li (Tsinghua University); Yanjie Li (Tsinghua University); Yalei Lv (Tsinghua University); Yong Jiang (Tsinghua University); Shutao Xia (Tsinghua University)
- <b>$\delta$-CLUE: Diverse Sets of Explanations for Uncertainty Estimates</b> <br /> Dan Ley (University of Cambridge); Umang Bhatt (University of Cambridge); Adrian Weller (University of Cambridge)
- <b>Safe Exploration Method for Reinforcement Learning under Existence of Disturbance</b> <br /> Yoshihiro Okawa (Fujitsu Laboratories Ltd.); Yusuke Kato (Keio University); Tomotake Sasaki (Fujitsu Laboratories Ltd.); Hitoshi Yanami (Fujitsu Laboratories Ltd.); Toru Namerikawa (Keio University)
- <b>Provable defense by denoised smoothing with learned score function</b> <br /> Kyungmin Lee (Agency for Defense Development)
- <b>Boosting black-box adversarial attack via exploiting loss smoothness</b> <br /> Hoang Tran (Oak Ridge National Lab); Dan Lu (Oak Ridge National Laboratory); Guannan Zhang (Oak Ridge National Laboratory)
- <b>PatchGuard++: Efficient Provable Attack Detection against Adversarial Patches</b> <br /> Chong Xiang (Princeton University); Prateek Mittal (Princeton University)
- <b>DEEP GRADIENT ATTACK WITH STRONG DP-SGD LOWER BOUND FOR LABEL PRIVACY</b> <br /> Sen Yuan (Facebook); Min Xue (Facebook); Kaikai Wang (Facebook); Milan Shen (Facebook)
- <b>Measuring Adversarial Robustness using a Voronoi-Epsilon Adversary</b> <br /> Hyeongji Kim (Institute of Marine Research); Pekka Parviainen (University of Bergen); Ketil Malde (UIB)
- <b>Accelerated Policy Evaluation with Adaptive Importance Sampling</b> <br /> Mengdi Xu (Carnegie Mellon University); Peide Huang (Carnegie Mellon University); Fengpei Li (Columbia University); Jiacheng Zhu (Carnegie Mellon University); ‪Xuewei (Tony) Qi (Toyota North America Research Labs); Zhiyuan Huang (Tongji University); Henry Lam (Columbia University); DING ZHAO (Carnegie Mellon University)
- <b>Sparse Coding Frontend for Robust Neural Networks</b> <br /> Can Bakiskan (University of California, Santa Barbara); Metehan Cekic (University of California, Santa Barbara); Ahmet D Sezer (University of California, Santa Barbara); Upamanyu Madhow (University of California, Santa Barbara)
- <b>Subnet Replacement: Deployment-stage backdoor attack against deep neural networks in gray-box setting</b> <br /> Xiangyu Qi (Zhejiang University); Jifeng Zhu (Tencent); Chulin Xie (University of Illinois at Urbana-Champaign); Yong Yang (Tencent)
- <b>Speeding Up Neural Network Verification via Automated Algorithm Configuration</b> <br /> Matthias König (Leiden University); Holger Hoos (Leiden Institute of Advanced Computer Science, Leiden University); Jan Van Rijn (Leiden University)
- <b>Incorporating Label Uncertainty in Intrinsic Robustness Measures</b> <br /> Xiao Zhang (University of Virginia); David Evans (University of Virginia)
- <b>FIRM: Detecting Adversarial Audios by Recursive Filters with Randomization</b> <br /> Guanhong Tao (Purdue University); Xiaowei Chen (Baidu X-Lab); Yunhan Jia (Bytedance Inc.); Zhenyu Zhong (Baidu); Shiqing Ma (Rutgers University); Xiangyu Zhang (Purdue University)
- <b>On Improving Adversarial Robustness Using Proxy Distributions</b> <br /> Vikash Sehwag (Princeton University); Saeed Mahloujifar (Princeton University); Sihui Dai (California Institute of Technology); Tinashe Handina (Princeton University); Chong Xiang (Princeton University); Mung Chiang (Purdue University); Prateek Mittal (Princeton University)
- <b>Detecting Adversarial Attacks through Neural Activations</b> <br /> Graham Annett (Boise State University); Hoda Mehrpouyan (Boise State University); Tim Andersen (Boise State); Casey R Kennington (Boise State University); Craig Primer (Boise State University)
- <b>Preventing Unauthorized Use of Proprietary Data: Poisoning for Secure Dataset Release</b> <br /> Liam Fowl (University of Maryland); Ping-yeh Chiang (University of Maryland, College Park); Micah Goldblum (University of Maryland, College Park); Jonas A. Geiping (University of Siegen); Arpit Bansal (University of Maryland - College Park); Wojciech Czaja (University of Maryland, College Park); Tom Goldstein (University of Maryland, College Park)
- <b>Robustness from Perception</b> <br /> Saeed Mahloujifar (Princeton University); Chong Xiang (Princeton University); Vikash Sehwag (Princeton University); Sihui Dai (California Institute of Technology); Prateek Mittal (Princeton University)
- <b>Mitigating Adversarial Training Instability with Batch Normalization</b> <br /> Arvind Sridhar (UC Berkeley); Chawin Sitawarin (UC Berkeley); David Wagner (UC Berkeley)
- <b>Fighting Gradients with Gradients: Dynamic Defenses against Adversarial Attacks</b> <br /> Dequan Wang (UC Berkeley); Evan Shelhamer (Imaginary Number); An Ju (University of California, Berkeley); David Wagner (UC Berkeley); Trevor Darrell (UC Berkeley)
- <b>Mind the box: $l_1$-APGD for sparse adversarial attacks on image classifiers</b> <br /> Francesco Croce (University of Tübingen); Matthias Hein (University of Tübingen)
- <b>DP-InstaHide: Provably Defusing Poisoning and Backdoor Attacks with Differentially Private Data Augmentations</b> <br /> Eitan Borgnia (University of Maryland); Jonas A. Geiping (University of Siegen); Valeriia Cherepanova (University of Maryland); Liam Fowl (University of Maryland); Arjun Gupta (University of Maryland College Park); Amin Ghiasi (University of Maryland); Furong Huang (University of Maryland); Micah Goldblum (University of Maryland); Tom Goldstein (University of Maryland, College Park)
- <b>RobustBench: a standardized adversarial robustness benchmark</b> <br /> Francesco Croce (University of Tübingen); Maksym Andriushchenko (EPFL); Vikash Sehwag (Princeton University); Edoardo Debenedetti (EPFL); Nicolas Flammarion (EPFL); Mung Chiang (Princeton University); Prateek Mittal (Princeton University); Matthias Hein (University of Tübingen)
- <b>Simple Transparent Adversarial Examples</b> <br /> Jaydeep Jitendra Borkar (Savitribai Phule Pune University); Pin-Yu Chen (IBM Research)
- <b>Moral Scenarios for Reinforcement Learning Agents</b> <br /> Dan Hendrycks (UC Berkeley); Mantas Mazeika (UIUC); Andy Zou (UC Berkeley); Sahil Patel (UC Berkeley); Christine Zhu (UC Berkeley); Jesus Navarro (UC Berkeley); Bo Li (UIUC); Dawn Song (UC Berkeley); Jacob Steinhardt (UC Berkeley)