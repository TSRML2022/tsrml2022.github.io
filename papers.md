---
layout: default
---

# Accepted Papers

- <b>[Efficient Disruptions of Black-box Image Translation Deepfake Generation Systems](https://aisecure-workshop.github.io/aml-iclr2021/papers/0.pdf)</b> <br /> Nataniel Ruiz (Boston University); Sarah Bargal (Boston University); Stan Sclaroff (Boston University)
- <b>[Bridging the Gap Between Adversarial Robustness and Optimization Bias](https://aisecure-workshop.github.io/aml-iclr2021/papers/1.pdf)</b> <br /> Fartash Faghri (University of Toronto); Cristina Vasconcelos (Google); David J Fleet (University of Toronto); Fabian Pedregosa (Google); Nicolas Le Roux (Google)
- <b>[Covariate Shift Adaptation for Adversarially Robust Classifier](https://aisecure-workshop.github.io/aml-iclr2021/papers/2.pdf)</b> <br /> Jay Nandy (National University of Singapore); Sudipan Saha (Technical University of Munich); Wynne Hsu (National University of Singapore); Mong Li Lee (National University of Singapore); Xiaoxiang Zhu (Technical University of Munich,Germany)
- <b>[Poisoned classifiers are not only backdoored, they are fundamentally broken](https://aisecure-workshop.github.io/aml-iclr2021/papers/3.pdf)</b> <br /> Mingjie Sun (Carnegie Mellon University); Siddhant Agarwal (Indian Institute of Technology, Kharagpur); Zico Kolter (Carnegie Mellon University)
- <b>[Reliably fast adversarial training via latent adversarial perturbation](https://aisecure-workshop.github.io/aml-iclr2021/papers/4.pdf)</b> <br /> Geon Yeong Park (KAIST); Sang Wan Lee (KAIST)
- <b>[Imbalanced Gradients: A New Cause of Overestimated Adversarial Robustness](https://aisecure-workshop.github.io/aml-iclr2021/papers/5.pdf)</b> <br /> Linxi Jiang (Fudan University); Xingjun Ma (Deakin University); Zejia Weng (Fudan University); James Bailey (THE UNIVERSITY OF MELBOURNE); Yu-Gang Jiang (Fudan University)
- <b>[SHIFT INVARIANCE CAN REDUCE ADVERSARIAL ROBUSTNESS](https://aisecure-workshop.github.io/aml-iclr2021/papers/6.pdf)</b> <br /> David Jacobs (University of Maryland, USA); Ronen Basri (Weizmann Institute of Science); Vasu Singla (University Of Maryland); Songwei Ge (University of Maryland)
- <b>[What is Wrong with One-Class Anomaly Detection?](https://aisecure-workshop.github.io/aml-iclr2021/papers/7.pdf)</b> <br /> JuneKyu Park (Ajou University); Jeong-Hyeon Moon (Ajou University); Namhyuk Ahn (Ajou University); Kyung-Ah Sohn (Ajou University)
- <b>[Safe Model-based Reinforcement Learning with Robust Cross-Entropy Method](https://aisecure-workshop.github.io/aml-iclr2021/papers/8.pdf)</b> <br /> Zuxin Liu (Carnegie Mellon University); Hongyi Zhou (CMU); Baiming Chen (Tsinghua University); Sicheng Zhong (University of Toronto); DING ZHAO (Carnegie Mellon University)
- <b>[GateNet: Bridging the gap between Binarized Neural Network and FHE evaluation](https://aisecure-workshop.github.io/aml-iclr2021/papers/9.pdf)</b> <br /> Cheng Fu (University of California, San Diego); Hanxian Huang (UC San Diego); Xinyun Chen (UC Berkeley); Jishen Zhao (University of California, San Diego)
- <b>[High-Robustness, Low-Transferability Fingerprinting of Neural Networks](https://aisecure-workshop.github.io/aml-iclr2021/papers/10.pdf)</b> <br /> Siyue Wang (Northeastern University); Xiao Wang (Boston University); Pin-Yu Chen (IBM Research); Pu Zhao (Northeastern University); Xue Lin (Northeastern University)
- <b>[Poisoning Deep Reinforcement Learning Agents with In-Distribution Triggers](https://aisecure-workshop.github.io/aml-iclr2021/papers/11.pdf)</b> <br /> Kiran Karra (JHU/APL); Chace Ashcraft (JHU/APL)
- <b>[Non-Singular Adversarial Robustness of Neural Networks](https://aisecure-workshop.github.io/aml-iclr2021/papers/12.pdf)</b> <br /> Yu-Lin Tsai (National Chiao Tung University); Chia-Yi Hsu (National Yang Ming Chiao Tung University); Chia-Mu Yu (National Chiao Tung University); Pin-Yu Chen (IBM Research)
- <b>[Adversarial Examples Make Stronger Poisons](https://aisecure-workshop.github.io/aml-iclr2021/papers/13.pdf)</b> <br /> Liam Fowl (University of Maryland); Micah Goldblum (University of Maryland, College Park); Ping-yeh Chiang (University of Maryland, College Park); Jonas A. Geiping (University of Siegen); Tom Goldstein (University of Maryland, College Park)
- <b>[What Doesn't Kill You Makes You Robust(er): Adversarial Training against Poisons and Backdoors](https://aisecure-workshop.github.io/aml-iclr2021/papers/14.pdf)</b> <br /> Jonas A. Geiping (University of Siegen); Liam Fowl (University of Maryland); Gowthami Somepalli (University of Maryland); Micah Goldblum (University of Maryland); Michael Moeller (University of Siegen); Tom Goldstein (University of Maryland, College Park)
- <b>[Byzantine-Robust and Privacy-Preserving Framework for FedML](https://aisecure-workshop.github.io/aml-iclr2021/papers/15.pdf)</b> <br /> Hanieh Hashemi (University of Southern California); Yongqin Wang (University of Southern California); Chuan Guo (Facebook AI Research); Murali Annavaram (University of Southern California)
- <b>[Data Augmentation Can Improve Robustness](https://aisecure-workshop.github.io/aml-iclr2021/papers/16.pdf)</b> <br /> Sylvestre-Alvise Rebuffi (DeepMind); Sven Gowal (DeepMind); Dan Andrei Calian (DeepMind); Florian Stimberg (); Olivia Wiles (DeepMind); Timothy Arthur Mann (DeepMind)
- <b>[Baseline Pruning-Based Approach to Trojan Detection in Neural Networks](https://aisecure-workshop.github.io/aml-iclr2021/papers/17.pdf)</b> <br /> Peter Bajcsy (NIST); Michael Majurski (NIST)
- <b>[Extracting Hyperparameter Constraints From Code](https://aisecure-workshop.github.io/aml-iclr2021/papers/18.pdf)</b> <br /> Ingkarat Rak-amnouykit (Rensselaer Polytechnic Institute); Ana Milanova (Rensselaer Polytechnic Institute); Guillaume Baudart (Inria Paris, École normale supérieure - PSL University); Martin Hirzel (IBM Research); Julian Dolby (IBM Research)
- <b>[Coordinated Attacks Against Federated Learning: A Multi-Agent Reinforcement Learning Approach](https://aisecure-workshop.github.io/aml-iclr2021/papers/19.pdf)</b> <br /> Wen Shen (Tulane University); Henger Li (Tulane University); Zizhan Zheng (Tulane University)
- <b>[Regularization Can Help Mitigate Poisoning Attacks... with the Right Hyperparameters](https://aisecure-workshop.github.io/aml-iclr2021/papers/20.pdf)</b> <br /> Javier Carnerero-Cano (Imperial College London); Luis Muñoz-González (Imperial College London); Phillippa Spencer (Defence Science and Technology Laboratory); Emil Lupu (Imperial College London)
- <b>[Ditto: Fair and Robust Federated Learning Through Personalization](https://aisecure-workshop.github.io/aml-iclr2021/papers/21.pdf)</b> <br /> Tian Li (Carnegie Mellon University); Shengyuan Hu (Carnegie Mellon University); Ahmad Beirami (Facebook AI); Virginia Smith (Carnegie Mellon University)
- <b>[Low Curvature Activations Reduce Overfitting in Adversarial Training](https://aisecure-workshop.github.io/aml-iclr2021/papers/22.pdf)</b> <br /> Vasu Singla (University Of Maryland); Sahil Singla (University of Maryland); David Jacobs (University of Maryland, USA); Soheil Feizi (University of Maryland)
- <b>[Examining Trends in Out-of-Domain Confidence](https://aisecure-workshop.github.io/aml-iclr2021/papers/23.pdf)</b> <br /> Hamza Qadeer (University of California, Berkeley); Michael Chau (University of California, Berkeley); Eric Zhu (University of California, Berkeley); Matthew A Wright (University of California Berkeley); Richard Liaw (UC Berkeley) | [YouTube presentation](https://www.youtube.com/watch?v=-r9rk5Gvip4)
- <b>[Doing More with Less: Improving Robustness using Generated Data](https://aisecure-workshop.github.io/aml-iclr2021/papers/24.pdf)</b> <br /> Sven Gowal (DeepMind); Sylvestre-Alvise Rebuffi (DeepMind); Olivia Wiles (DeepMind); Florian Stimberg (); Dan Andrei Calian (DeepMind); Timothy Arthur Mann (DeepMind)
- <b>[Hidden Backdoor Attack against Semantic Segmentation Models](https://aisecure-workshop.github.io/aml-iclr2021/papers/25.pdf)</b> <br /> Yiming Li (Tsinghua University); Yanjie Li (Tsinghua University); Yalei Lv (Tsinghua University); Yong Jiang (Tsinghua University); Shutao Xia (Tsinghua University)
- <b>[$\delta$-CLUE: Diverse Sets of Explanations for Uncertainty Estimates](https://aisecure-workshop.github.io/aml-iclr2021/papers/26.pdf)</b> <br /> Dan Ley (University of Cambridge); Umang Bhatt (University of Cambridge); Adrian Weller (University of Cambridge)
- <b>[Safe Exploration Method for Reinforcement Learning under Existence of Disturbance](https://aisecure-workshop.github.io/aml-iclr2021/papers/27.pdf)</b> <br /> Yoshihiro Okawa (Fujitsu Laboratories Ltd.); Yusuke Kato (Keio University); Tomotake Sasaki (Fujitsu Laboratories Ltd.); Hitoshi Yanami (Fujitsu Laboratories Ltd.); Toru Namerikawa (Keio University)
- <b>[Provable defense by denoised smoothing with learned score function](https://aisecure-workshop.github.io/aml-iclr2021/papers/28.pdf)</b> <br /> Kyungmin Lee (Agency for Defense Development)
- <b>[Boosting black-box adversarial attack via exploiting loss smoothness](https://aisecure-workshop.github.io/aml-iclr2021/papers/29.pdf)</b> <br /> Hoang Tran (Oak Ridge National Lab); Dan Lu (Oak Ridge National Laboratory); Guannan Zhang (Oak Ridge National Laboratory)
- <b>[PatchGuard++: Efficient Provable Attack Detection against Adversarial Patches](https://aisecure-workshop.github.io/aml-iclr2021/papers/30.pdf)</b> <br /> Chong Xiang (Princeton University); Prateek Mittal (Princeton University)
- <b>[DEEP GRADIENT ATTACK WITH STRONG DP-SGD LOWER BOUND FOR LABEL PRIVACY](https://aisecure-workshop.github.io/aml-iclr2021/papers/31.pdf)</b> <br /> Sen Yuan (Facebook); Min Xue (Facebook); Kaikai Wang (Facebook); Milan Shen (Facebook)
- <b>[Measuring Adversarial Robustness using a Voronoi-Epsilon Adversary](https://aisecure-workshop.github.io/aml-iclr2021/papers/32.pdf)</b> <br /> Hyeongji Kim (Institute of Marine Research); Pekka Parviainen (University of Bergen); Ketil Malde (UIB)
- <b>[Accelerated Policy Evaluation with Adaptive Importance Sampling](https://aisecure-workshop.github.io/aml-iclr2021/papers/33.pdf)</b> <br /> Mengdi Xu (Carnegie Mellon University); Peide Huang (Carnegie Mellon University); Fengpei Li (Columbia University); Jiacheng Zhu (Carnegie Mellon University); ‪Xuewei (Tony) Qi (Toyota North America Research Labs); Zhiyuan Huang (Tongji University); Henry Lam (Columbia University); DING ZHAO (Carnegie Mellon University)
- <b>[Sparse Coding Frontend for Robust Neural Networks](https://aisecure-workshop.github.io/aml-iclr2021/papers/34.pdf)</b> <br /> Can Bakiskan (University of California, Santa Barbara); Metehan Cekic (University of California, Santa Barbara); Ahmet D Sezer (University of California, Santa Barbara); Upamanyu Madhow (University of California, Santa Barbara)
- <b>[Subnet Replacement: Deployment-stage backdoor attack against deep neural networks in gray-box setting](https://aisecure-workshop.github.io/aml-iclr2021/papers/35.pdf)</b> <br /> Xiangyu Qi (Zhejiang University); Jifeng Zhu (Tencent); Chulin Xie (University of Illinois at Urbana-Champaign); Yong Yang (Tencent)
- <b>[Speeding Up Neural Network Verification via Automated Algorithm Configuration](https://aisecure-workshop.github.io/aml-iclr2021/papers/36.pdf)</b> <br /> Matthias König (Leiden University); Holger Hoos (Leiden Institute of Advanced Computer Science, Leiden University); Jan Van Rijn (Leiden University)
- <b>[Incorporating Label Uncertainty in Intrinsic Robustness Measures](https://aisecure-workshop.github.io/aml-iclr2021/papers/37.pdf)</b> <br /> Xiao Zhang (University of Virginia); David Evans (University of Virginia)
- <b>[FIRM: Detecting Adversarial Audios by Recursive Filters with Randomization](https://aisecure-workshop.github.io/aml-iclr2021/papers/38.pdf)</b> <br /> Guanhong Tao (Purdue University); Xiaowei Chen (Baidu X-Lab); Yunhan Jia (Bytedance Inc.); Zhenyu Zhong (Baidu); Shiqing Ma (Rutgers University); Xiangyu Zhang (Purdue University)
- <b>[On Improving Adversarial Robustness Using Proxy Distributions](https://aisecure-workshop.github.io/aml-iclr2021/papers/39.pdf)</b> <br /> Vikash Sehwag (Princeton University); Saeed Mahloujifar (Princeton University); Sihui Dai (California Institute of Technology); Tinashe Handina (Princeton University); Chong Xiang (Princeton University); Mung Chiang (Purdue University); Prateek Mittal (Princeton University)
- <b>[Detecting Adversarial Attacks through Neural Activations](https://aisecure-workshop.github.io/aml-iclr2021/papers/40.pdf)</b> <br /> Graham Annett (Boise State University); Hoda Mehrpouyan (Boise State University); Tim Andersen (Boise State); Casey R Kennington (Boise State University); Craig Primer (Boise State University)
- <b>[Preventing Unauthorized Use of Proprietary Data: Poisoning for Secure Dataset Release](https://aisecure-workshop.github.io/aml-iclr2021/papers/41.pdf)</b> <br /> Liam Fowl (University of Maryland); Ping-yeh Chiang (University of Maryland, College Park); Micah Goldblum (University of Maryland, College Park); Jonas A. Geiping (University of Siegen); Arpit Bansal (University of Maryland - College Park); Wojciech Czaja (University of Maryland, College Park); Tom Goldstein (University of Maryland, College Park)
- <b>[Robustness from Perception](https://aisecure-workshop.github.io/aml-iclr2021/papers/42.pdf)</b> <br /> Saeed Mahloujifar (Princeton University); Chong Xiang (Princeton University); Vikash Sehwag (Princeton University); Sihui Dai (California Institute of Technology); Prateek Mittal (Princeton University)
- <b>[Mitigating Adversarial Training Instability with Batch Normalization](https://aisecure-workshop.github.io/aml-iclr2021/papers/43.pdf)</b> <br /> Arvind Sridhar (UC Berkeley); Chawin Sitawarin (UC Berkeley); David Wagner (UC Berkeley)
- <b>[Fighting Gradients with Gradients: Dynamic Defenses against Adversarial Attacks](https://aisecure-workshop.github.io/aml-iclr2021/papers/44.pdf)</b> <br /> Dequan Wang (UC Berkeley); Evan Shelhamer (Imaginary Number); An Ju (University of California, Berkeley); David Wagner (UC Berkeley); Trevor Darrell (UC Berkeley)
- <b>[Mind the box: $l_1$-APGD for sparse adversarial attacks on image classifiers](https://aisecure-workshop.github.io/aml-iclr2021/papers/45.pdf)</b> <br /> Francesco Croce (University of Tübingen); Matthias Hein (University of Tübingen)
- <b>[DP-InstaHide: Provably Defusing Poisoning and Backdoor Attacks with Differentially Private Data Augmentations](https://aisecure-workshop.github.io/aml-iclr2021/papers/46.pdf)</b> <br /> Eitan Borgnia (University of Maryland); Jonas A. Geiping (University of Siegen); Valeriia Cherepanova (University of Maryland); Liam Fowl (University of Maryland); Arjun Gupta (University of Maryland College Park); Amin Ghiasi (University of Maryland); Furong Huang (University of Maryland); Micah Goldblum (University of Maryland); Tom Goldstein (University of Maryland, College Park)
- <b>[RobustBench: a standardized adversarial robustness benchmark](https://aisecure-workshop.github.io/aml-iclr2021/papers/47.pdf)</b> <br /> Francesco Croce (University of Tübingen); Maksym Andriushchenko (EPFL); Vikash Sehwag (Princeton University); Edoardo Debenedetti (EPFL); Nicolas Flammarion (EPFL); Mung Chiang (Princeton University); Prateek Mittal (Princeton University); Matthias Hein (University of Tübingen)
- <b>[Simple Transparent Adversarial Examples](https://aisecure-workshop.github.io/aml-iclr2021/papers/48.pdf)</b> <br /> Jaydeep Jitendra Borkar (Savitribai Phule Pune University); Pin-Yu Chen (IBM Research)
- <b>[Moral Scenarios for Reinforcement Learning Agents](https://aisecure-workshop.github.io/aml-iclr2021/papers/49.pdf)</b> <br /> Dan Hendrycks (UC Berkeley); Mantas Mazeika (UIUC); Andy Zou (UC Berkeley); Sahil Patel (UC Berkeley); Christine Zhu (UC Berkeley); Jesus Navarro (UC Berkeley); Bo Li (UIUC); Dawn Song (UC Berkeley); Jacob Steinhardt (UC Berkeley)