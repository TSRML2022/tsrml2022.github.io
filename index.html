<!DOCTYPE html>
<html lang="en-US">
  <head>

    
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Trustworthy and Socially Responsible Machine Learning | Workshop at NeurIPS 2022</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Trustworthy and Socially Responsible Machine Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Trustworthy and Socially Responsible Machine Learning | Workshop at NeurIPS 2022" />
<meta property="og:description" content="Trustworthy and Socially Responsible Machine Learning | Workshop at NeurIPS 2022" />
<meta property="og:site_name" content="Trustworthy and Socially Responsible Machine Learning" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Trustworthy and Socially Responsible Machine Learning" />
<script type="application/ld+json">
{"headline":"Trustworthy and Socially Responsible Machine Learning","url":"/","name":"Trustworthy and Socially Responsible Machine Learning","description":"Trustworthy and Socially Responsible Machine Learning | Workshop at NeurIPS 2022","@type":"WebSite","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=">
  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">TSRML <br> Trustworthy and Socially Responsible <br> Machine Learning</h1>
      <h2 class="project-tagline">Workshop at <a href="https://neurips.cc/">NeurIPS 2022</a><br> Virtual, December 9, 2022</h2>
      
      
      <a href="/" class="btn">Home</a>
      
      <a href="/cfp" class="btn">Call for Papers</a>
      
<!--       <a href="/papers" class="btn">Accepted Papers</a> -->
      
      <a href="/schedule" class="btn">Tentative Schedule</a>
      
      <a href="/speakers" class="btn">Speakers</a>
      
      <a href="/organizers" class="btn">Organizers</a>
      
      <a href="/committee" class="btn">Program Committee</a>
      
      <a href="/related" class="btn">Related Workshops</a>
      
    </header>

    <main id="content" class="main-content" role="main">
      <h1 id="overview">Overview</h1>

<table>
  <tbody>
    <tr>
      <td><strong>Date</strong></td>
      <td>December 9, 2022</td>
    </tr>
    <tr>
      <td><strong>Location</strong></td>
	    <td>Virtual</td>
<!--       <td>The workshop will be held <em>virtually</em>. The internal NeurIPS workshop website is <a href="https://iclr.cc/virtual/2021/workshop/2127">here</a> (NeurIPS registration required).</td> -->
    </tr>
  </tbody>
</table>

<p>While machine learning (ML) models have achieved great success in many applications, concerns have been raised about their potential security, privacy, fairness, transparency and ethics issues when applied to real-world applications. Irresponsibly applying machine learning to mission-critical and human-centric domains such as healthcare, education, and law can lead to serious misuse, inequity issues, negative economic and environmental impacts, and/or legal and ethical concerns. For example, existing research has well documented that a machine learning model can exhibit discrimination against already-disadvantaged or marginalized social groups, such as BIPOC and LGBTQ+; moreover, it has been demonstrated that a machine learning model may unintentially leak sensitve personal information such as medical records; last but not least, machine learning models are often regarded as “blackboxes” and can produce unreliable, unpredictable and unexplanable outcomes especially under domain-shifts or macliciously crafted attacks.</p>

<p>To address these negative societal impacts of ML, researchers have looked into different principles and constraints to ensure trustworthy and socially responsible machine learning systems. This workshop makes the first attempt towards bridging the gap between security, privacy, fairness, ethics, game theory, and machine learning communities and aims to discuss the principles and experiences of developing trustworthy and socially responsible machine learning systems. The workshop also focuses on how future researchers and practitioners should prepare themselves for reducing the risks of unintended behaviors of sophisticated ML models.</p>

<p>This workshop aims to bring together researchers interested in the emerging and interdisciplinary field of trustworthy and socially responsible machine learning from a broad range of disciplines with different perspectives to this problem. We attempt to highlight recent related work from different communities, clarify the foundations of trustworthy machine learning, and chart out important directions for future work and cross-community collaborations. Topics of this workshop include but are not limited to:</p>

<ul>
<li>Novel methods for building more trustworthy machine learning models that prevent or alleviate negative societal impacts of existing ML methods</li>
<li>New applications and settings where trustworthiness of machine learning plays an important role and how well existing techniques work under these settings</li>
<li>Machine learning models with verifiable guarantees (such as robustness, fairness and privacy guarantees) to build trustworthiness</li>
<li>Privacy-preserving machine learning approaches</li>
<li>Theoretical understanding of trustworthy machine learning</li>
<li>Explainable and interpretable AI</li>
<li>Robust decision making under uncertainty</li>
<li>Futuristic concerns about trustworthy machine learning</li>
<li>Game-theoretic analysis for socially responsible machine learning systems</li>
<li>Case studies and field research of the societal impacts of applying machine learning in mission-critical and human-centric tasks</li>
</ul>

<p><b>Points of difference</b>: This workshop aims to raise awareness of the societal issues involved in applying machine learning to real-world systems, and stimulate interdisciplinary research that can tackle open challenges on building trustworthy and socially responsible machine learning models. Although some of our topics may overlap with other workshops (e.g., attacks on ML are discussed in workshops on adversarial robustness), these workshops do not have a central theme on the social responsibility of machine learning and do not aim to directly address societal issues of ML. </p>

<h4>Diversity Statement</h4>
We have taken various steps to expand the diversity of the participants. The organizers and invited speakers have diverse backgrounds (e.g., gender, race, affiliations, seniority, and nationality). In particular, many are from underrepresented groups in STEM fields: the organizers include female scholars and the confirmed speakers include female scholars as well as researchers from underrepresented races. The organizers include both experienced and senior members as well as junior researchers, including people who have not organized this workshop series. Moreover, The workshop encompasses researchers from both industry and academia. Additionally, the workshop is inclusive and covers a wide range of topics (e.g., fairness, transparency, interpretability, privacy, robustness, etc.), which has raised a lot of attention now. We also aim to bring together both theoretical and applied researchers from various domains.

<!-- <h1 id="paper-awards">Paper Awards</h1> -->

<!-- <h2 id="best-paper-award">Best Paper Award</h2> -->

<!-- <ul>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/21.pdf">Ditto: Fair and Robust Federated Learning Through Personalization</a></b> <br /> Tian Li (Carnegie Mellon University); Shengyuan Hu (Carnegie Mellon University); Ahmad Beirami (Facebook AI); Virginia Smith (Carnegie Mellon University)</li>
</ul>

<h2 id="best-paper-honorable-mention-award">Best Paper Honorable Mention Award</h2>

<ul>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/47.pdf">RobustBench: a standardized adversarial robustness benchmark</a></b> <br /> Francesco Croce (University of Tübingen); Maksym Andriushchenko (EPFL); Vikash Sehwag (Princeton University); Edoardo Debenedetti (EPFL); Nicolas Flammarion (EPFL); Mung Chiang (Princeton University); Prateek Mittal (Princeton University); Matthias Hein (University of Tübingen)</li>
</ul> -->

<h2 id="organizers">Organizers</h2>

<ul>
  <li>Huan Zhang (Carnegie Mellon University)</li>
  <li>Linyi Li (University of Illinois Urbana-Champaign)</li>
  <li>Chaowei Xiao (Arizona State University, NVIDIA)</li>
  <li>Zico Kolter (Carnegie Mellon University)</li>
  <li>Anima Anandkumar (Caltech/NVIDIA)</li>
  <li>Bo Li (University of Illinois Urbana-Champaign)</li>
</ul>


      <footer class="site-footer">
        <span class="site-footer-credits">
          <!-- Please contact <a href="mailto:huan@huan-zhang.com">Huan Zhang</a> or <a href="mailto:linyi2@illinois.edu">Linyi Li</a> if you have any questions.<br> -->
          If you have any questions, please contact the organizers via <a href="mailto:tsrml2022@googlegroups.com">tsrml2022@googlegroups.com</a>.<br>
          This page was generated by <a href="https://pages.github.com">GitHub Pages</a> following the template of <a href="https://aisecure-workshop.github.io/aml-iclr2021">AML-ICLR 2021 workshop</a>.
        </span>
      </footer>
    </main>
  </body>
</html>
