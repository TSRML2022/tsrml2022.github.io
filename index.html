<!DOCTYPE html>
<html lang="en-US">
  <head>

    
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Trustworthy and Socially Responsible Machine Learning | Proposed Workshop at ICLR 2021</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Trustworthy and Socially Responsible Machine Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Proposed Workshop at NeurIPS 2022" />
<meta property="og:description" content="Proposed Workshop at NeurIPS 2022" />
<meta property="og:site_name" content="Trustworthy and Socially Responsible Machine Learning" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Trustworthy and Socially Responsible Machine Learning" />
<script type="application/ld+json">
{"headline":"Trustworthy and Socially Responsible Machine Learning","url":"/","name":"Trustworthy and Socially Responsible Machine Learning","description":"Proposed Workshop at NeurIPS 2022","@type":"WebSite","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=">
  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">Trustworthy and Socially Responsible Machine Learning</h1>
      <h2 class="project-tagline">Proposed Workshop at NeurIPS 2022</h2>
      
      
      <a href="/" class="btn">Home</a>
      
<!--       <a href="/cfp" class="btn">Call for Papers</a> -->
      
<!--       <a href="/papers" class="btn">Accepted Papers</a> -->
      
<!--       <a href="/schedule" class="btn">Schedule</a> -->
      
<!--       <a href="/speakers" class="btn">Speakers</a> -->
      
<!--       <a href="/organizers" class="btn">Organizers</a> -->
      
<!--       <a href="/committee" class="btn">Program Committee</a> -->
      
<!--       <a href="/related" class="btn">Related Workshops</a> -->
      
    </header>

    <main id="content" class="main-content" role="main">
      <h1 id="overview">Overview</h1>

<table>
  <tbody>
    <tr>
      <td><strong>Date</strong></td>
      <td>November 29, 2022</td>
    </tr>
    <tr>
      <td><strong>Location</strong></td>
	    <td>To be announced</td>
<!--       <td>The workshop will be held <em>virtually</em>. The internal ICLR workshop website is <a href="https://iclr.cc/virtual/2021/workshop/2127">here</a> (ICLR registration required).</td> -->
    </tr>
  </tbody>
</table>

<p>While machine learning (ML) models have achieved great success in many applications, concerns have been raised about their potential security, privacy, and fairness issues when applied to real-world applications. On the one hand, from the security perspective, studies have been conducted to explore worst-case attacks against ML models and therefore inspire both empirical and certifiable defense approaches. On the other hand, from the learning perspective, in the “data-centric AI” era, researchers have looked into different principles and constraints to ensure trustworthy and socially responsible machine learning systems. This workshop makes the first attempt towards bridging the gap between security, privacy, game theory, and machine learning communities and aims to discuss the principles and experiences of developing trustworthy and socially responsible machine learning systems. The workshop also focuses on how future researchers and practitioners should prepare themselves for reducing the risks of unintended behaviors of sophisticated ML models. </p>

<p>The workshop will bring together experts from machine learning, computer security, privacy, and game theory communities. We attempt to highlight recent related work from different communities, clarify the foundations of trustworthy machine learning, and chart out important directions for future work and cross-community collaborations.</p>


Topics include but are not limited to:
<ul>
	<li>Adversarial attacks against ML systems, including training and test time attacks</li>
<li>Improving model robustness against attacks, and empirical defenses</li>
<li>Certifiable defenses on machine learning models</li>
<li>Privacy-preserving machine learning approaches</li>
<li>Theoretical understanding of adversarial machine learning</li>
<li>Explainable and interpretable AI</li>
<li>Robust decision making under uncertainty</li>
<li>Futuristic concerns about trustworthy machine learning</li>
<li>Game-theoretic analysis for socially responsible machine learning systems</li>
</ul>
	   

<!-- <h1 id="paper-awards">Paper Awards</h1> -->

<!-- <h2 id="best-paper-award">Best Paper Award</h2> -->

<!-- <ul>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/21.pdf">Ditto: Fair and Robust Federated Learning Through Personalization</a></b> <br /> Tian Li (Carnegie Mellon University); Shengyuan Hu (Carnegie Mellon University); Ahmad Beirami (Facebook AI); Virginia Smith (Carnegie Mellon University)</li>
</ul>

<h2 id="best-paper-honorable-mention-award">Best Paper Honorable Mention Award</h2>

<ul>
  <li><b><a href="https://aisecure-workshop.github.io/aml-iclr2021/papers/47.pdf">RobustBench: a standardized adversarial robustness benchmark</a></b> <br /> Francesco Croce (University of Tübingen); Maksym Andriushchenko (EPFL); Vikash Sehwag (Princeton University); Edoardo Debenedetti (EPFL); Nicolas Flammarion (EPFL); Mung Chiang (Princeton University); Prateek Mittal (Princeton University); Matthias Hein (University of Tübingen)</li>
</ul> -->

<h2 id="organizers">Organizers</h2>

<ul>
  <li>Huan Zhang (Carnegie Mellon University)</li>
  <li>Linyi Li (University of Illinois Urbana-Champaign)</li>
  <li>Chaowei Xiao (Arizona State University, NVIDIA)</li>
  <li>Zico Kolter (Carnegie Mellon University)</li>
  <li>Bo Li (University of Illinois Urbana-Champaign)</li>
</ul>
<!-- 
<p>The workshop is sponsored by <a href="https://www.openphilanthropy.org/">Open Philanthropy</a>. The funding covers a Best Paper Award ($1,000), a Best Paper Honorable Mention Award ($500), and multiple travel grants (complimentary ICLR conference registrations).</p>

<p align="center">
	<img src="./assets/images/OpenPhil.png" alt="Open Phil" width="350" />
</p> -->


      <footer class="site-footer">
<!--         <span class="site-footer-credits">Please contact <a href="mailto:xinyun.chen@berkeley.edu">Xinyun Chen</a> or <a href="mailto:cihangxie306@gmail.com">Cihang Xie</a> if you have any questions.<br> -->
          This page was generated by <a href="https://pages.github.com">GitHub Pages</a> following the template of <a href="https://aisecure-workshop.github.io/aml-iclr2021">AML-ICLR 2021 workshop</a>.
        </span>
      </footer>
    </main>
  </body>
</html>
