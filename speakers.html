<!DOCTYPE html>
<html lang="en-US">
  <head>

    
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Trustworthy and Socially Responsible Machine Learning | Workshop at NeurIPS 2022</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Trustworthy and Socially Responsible Machine Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Trustworthy and Socially Responsible Machine Learning | Workshop at NeurIPS 2022" />
<meta property="og:description" content="Trustworthy and Socially Responsible Machine Learning | Workshop at NeurIPS 2022" />
<meta property="og:site_name" content="Trustworthy and Socially Responsible Machine Learning" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Trustworthy and Socially Responsible Machine Learning" />
<script type="application/ld+json">
{"headline":"Trustworthy and Socially Responsible Machine Learning","url":"/schedule.html","description":"Trustworthy and Socially Responsible Machine Learning | Workshop at NeurIPS 2022","@type":"WebPage","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=">
  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">Trustworthy and Socially Responsible <br> Machine Learning (TSRML)</h1>
      <h2 class="project-tagline">Workshop at <a href="https://neurips.cc/">NeurIPS 2022</a><br> Virtual, December 9, 2022</h2>
      
      
      <a href="/" class="btn">Home</a>
      
      <a href="/cfp" class="btn">Call for Papers</a>
      
      <a href="/papers" class="btn">Accepted Papers</a>
      
      <a href="/schedule" class="btn">Schedule</a>
      
      <a href="/speakers" class="btn">Speakers</a>
      
      <a href="/organizers" class="btn">Organizers</a>
      
      <a href="/committee" class="btn">Program Committee</a>
      
      <a href="/related" class="btn">Related Workshops</a>
      
    </header>

    <main id="content" class="main-content" role="main">
      <h1 id="schedule">Speakers</h1>

<p>Speakers are in alphabetical order by last name.</p>

<table>
  <tbody>
    <tr>
      <td><img src="./assets/images/kamalika.jpeg" alt="Kamalika Chaudhuri" width="225" /></td>
      <td><img src="./assets/images/nika.jpg" alt="Nika Haghtalab" width="225" /></td>
      <!-- <td><img src="./assets/images/somesh.jpeg" alt="Somesh Jha" width="180" /></td> -->
      <td><img src="./assets/images/Been_Kim.png" alt="Been Kim" width="225" /></td>
      <td><img src="./assets/images/YiMa.jpg" alt="Yi Ma" width="225" /></td>
    </tr>
    <tr>
      <td width="225" ><a href="https://cseweb.ucsd.edu/~kamalika/">Kamalika Chaudhuri</a><br />University of California, San Diego</td>
      <td width="225" ><a href="https://people.eecs.berkeley.edu/~nika/">Nika Haghtalab</a><br />University of California, Berkeley</td>
      <!-- <td width="180" ><a href="https://pages.cs.wisc.edu/~jha/">Somesh Jha</a><br />University of Wisconsin-Madison</td> -->
      <td width="225" ><a href="https://beenkim.github.io/">Been Kim</a><br />Google Brain</td>
      <td width="225" ><a href="https://people.eecs.berkeley.edu/~yima/">Yi Ma</a><br />University of California, Berkeley</td>
    </tr>

    <tr>
      <td><img src="./assets/images/Aleksander_Madry.png" alt="Aleksander Mądry" width="225" /></td>
      <td><img src="./assets/images/marco.jpg" alt="Marco Pavone" width="225" /></td>
      <!-- <td><img src="./assets/images/radha.jpg" alt="Radha Poovendran" width="180"/></td> -->
      <td><img src="./assets/images/dorsa.jpg" alt="Dorsa Sadigh" width="225"/></td>
      <td><img src="./assets/images/milind.jpeg" alt="Milind Tambe" width="225"/></td>
    </tr>
    <tr>
      <td width="225" ><a href="https://madry.mit.edu/">Aleksander Mądry</a><br />Massachusetts Institute of Technology</td>
      <td width="225" ><a href="https://profiles.stanford.edu/marco-pavone">Marco Pavone</a><br />Stanford University</td>
      <!-- <td width="180" ><a href="https://people.ece.uw.edu/radha/index.html">Radha Poovendran</a><br />University of Washington</td> -->
      <td width="225" ><a href="https://dorsa.fyi/">Dorsa Sadigh</a><br />Stanford University</td>
      <td width="225" ><a href="https://teamcore.seas.harvard.edu/tambe">Milind Tambe</a><br />Harvard University</td>
    </tr>
  </tbody>
</table>

<h2>Talk Abstracts</h2>

  <h4>Aleksander Mądry (09:00 - 09:30): Towards a Distribution Shift Mitigation Toolkit</h4>

  <p>For all the progress in this context, modern machine learning models remain remarkably brittle to distribution shift. Can we thus build a reliable toolkit for discovery (and mitigation) of such model failures in a systematic and automated way?</p>

  <p>In this talk, we will present two frameworks advancing this goal. The first one is a scalable method for automatically distilling a model's failure modes as directions in the latent space. This method also enables leveraging these directions to perform synthetic data augmentations that specifically target (and thus help mitigate) these failures. The second framework constitutes a general approach to performing data-centric comparison of different models.</p>

  <h4>Milind Tambe (10:00 - 10:30): To be announced</h4>

  <h4>Nika Haghtalab (11:30 - noon): Multi-distribution learning, for robustness, fairness, and collaboration</h4>

  <p>Social and real-world considerations such as robustness, fairness, social welfare, and multi-agent tradeoffs have given rise to multi-distribution learning paradigms. In recent years, these paradigms have been studied by several disconnected communities and under different names, including collaborative learning, distributional robustness, and fair federated learning. In this talk, I will highlight the importance of multi-distribution learning paradigms in general, introduce technical tools for addressing them, and discuss how these problems relate to classical and modern consideration in data driven processes.</p>

  <h4>Kamalika Chaudhuri (noon - 12:30): Can we Refute Membership inference?</h4>

  <p>Membership inference (MI) predicts whether a data point was used for training a machine learning (ML) model, and is currently the most widely deployed attack for auditing privacy of model. In this work, we take a closer look at the Membership Inference attack and whether it is plausibly deniable, and show how to construct a Proof-of-Repudiation (PoR) which empowers the dataset owner to plausibly repudiate the predictions of an MI attack. This casts a doubt on the reliability of MI attacks in practice. Our empirical evaluations show that it is possible to construct PoRs efficiently for neural networks on standard datasets. </p>

  <p>Joint work with Zhifeng Kong and Amrita RoyChowdhury</p>

  <h4>Been Kim (12:30 - 13:00): To be announced</h4>

  <h4>Yi Ma (14:00 - 14:30): To be announced</h4>

  <h4>Dorsa Sadigh (14:30 - 15:00): Aligning Robot Representations with Humans</h4>

  <p>Aligning robot objectives with human preferences is a key challenge in robot learning. In this talk, I will start with discussing how active learning of human preferences can effectively query humans with the most informative questions to learn their preference reward functions. I will discuss some of the limitations of prior work, and how approaches such as few-shot learning can be integrated with active preference based learning for the goal of reducing the number of queries to a human expert and allowing for truly bringing in humans in the loop of learning neural reward functions. I will then talk about how we could go beyond active learning from a single human, and tap into large language models (LLMs) as another source of information to capture human preferences that are hard to specify. I will discuss how LLMs can be queried within a reinforcement learning loop and help with reward design. Finally I will discuss how the robot can also provide useful information to the human and be more transparent about its learning process. We demonstrate how the robot’s transparent behavior would guide the human to provide compatible demonstrations that are more useful and informative for learning.</p>

  <h4>Marco Pavone (15:00 - 15:30): Run-time monitoring for safe robot autonomy</h4>

  <p>In this talk I will present our recent results towards designing run-time monitors that can equip any pre-trained deep neural network with a task-relevant epistemic uncertainty estimate. I will show how run-time monitors can be used to identify, in real-time, anomalous inputs and, more broadly, provide safety assurances for learning-based autonomy stacks. Finally, I will discuss how run-time monitors can also be used to devise effective strategies for data lifecycle management.</p>


      <footer class="site-footer">
        <span class="site-footer-credits">
          <!-- Please contact <a href="mailto:huan@huan-zhang.com">Huan Zhang</a> or <a href="mailto:linyi2@illinois.edu">Linyi Li</a> if you have any questions.<br> -->
          If you have any questions, please contact the organizers via <a href="mailto:tsrml2022@googlegroups.com">tsrml2022@googlegroups.com</a>.<br>
          This page was generated by <a href="https://pages.github.com">GitHub Pages</a> following the template of <a href="https://aisecure-workshop.github.io/aml-iclr2021">AML-ICLR 2021 workshop</a>.
        </span>
      </footer>
    </main>
  </body>
</html>
